{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import io\n",
    "import itertools\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load CSV file\n",
    "# csv = \"butterflies/Training_set.csv\"\n",
    "# images = \"butterflies/train\"\n",
    "\n",
    "# df = pd.read_csv(csv)\n",
    "\n",
    "# # Generate full image paths\n",
    "# df['image_path'] = df['filename'].apply(lambda x: os.path.join(images, x))\n",
    "\n",
    "# # Convert labels to categorical if needed\n",
    "# labels = df['label'].values\n",
    "# image_paths = df['image_path'].values\n",
    "\n",
    "# # Encode labels if they are categorical (string)\n",
    "# label_encoder = LabelEncoder()\n",
    "# labels = label_encoder.fit_transform(labels)\n",
    "# df['label_encoded'] = labels\n",
    "\n",
    "# number_of_labels = len(set(labels))\n",
    "# number_of_labels\n",
    "\n",
    "# train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "#     df['image_path'], df['label_encoded'], test_size=0.2, stratify=df['label_encoded'], random_state=42\n",
    "# )\n",
    "# val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
    "#     test_paths, test_labels, test_size=0.5, stratify=test_labels, random_state=42\n",
    "# )\n",
    "\n",
    "# IMG_X = 224\n",
    "# IMG_Y = 224\n",
    "\n",
    "# def load_image(image_path, label):\n",
    "#     image = tf.io.read_file(image_path)\n",
    "#     image = tf.image.decode_jpeg(image, channels=3)\n",
    "#     image = tf.image.resize(image, (IMG_X, IMG_Y)) / 255.0  # Normalize\n",
    "#     return image, label\n",
    "\n",
    "# # Shuffle, batch, and prefetch\n",
    "# BATCH_SIZE = 32\n",
    "\n",
    "# # Convert lists into a TensorFlow dataset\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "# train_dataset = train_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# train_dataset = train_dataset.shuffle(len(train_paths)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "# val_dataset = val_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
    "# test_dataset = test_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13642 files belonging to 14 classes.\n",
      "Using 9550 files for training.\n",
      "Number of classes: 14\n",
      "Class names: ['astilbe', 'bellflower', 'black_eyed_susan', 'calendula', 'california_poppy', 'carnation', 'common_daisy', 'coreopsis', 'dandelion', 'iris', 'rose', 'sunflower', 'tulip', 'water_lily']\n",
      "Found 13642 files belonging to 14 classes.\n",
      "Using 4092 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "data_dir = pathlib.Path(\"flowers\")  # Replace with your actual path\n",
    "\n",
    "# Image settings\n",
    "BATCH_SIZE = 32\n",
    "IMG_X = 256\n",
    "IMG_Y = 256\n",
    "IMG_SIZE = (IMG_X, IMG_Y)  # Matches your image size\n",
    "\n",
    "# Load training dataset (70%)\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.3,  # 30% for val+test\n",
    "    subset=\"training\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "number_of_labels = len(train_dataset.class_names)  # Get number of unique labels\n",
    "print(f\"Number of classes: {number_of_labels}\")\n",
    "print(f\"Class names: {train_dataset.class_names}\")\n",
    "\n",
    "val_test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.3,  # 30% reserved for val+test\n",
    "    subset=\"validation\",  # This will contain val+test (30%)\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Step 2: Split val_test_dataset into Validation (15%) and Test (15%)\n",
    "val_batches = int(0.5 * len(val_test_dataset))  # 50% of val+test for validation\n",
    "\n",
    "val_dataset = val_test_dataset.take(val_batches)  # First half for validation\n",
    "test_dataset = val_test_dataset.skip(val_batches)  # Second half for testing\n",
    "\n",
    "# Prefetch data for better performance\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some constants/hyperparameters\n",
    "# BUFFER_SIZE = 70_000 # for reshuffling\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Conv2D(kernel_size=3, filters=32, activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(number_of_labels)\n",
    "# ])\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(IMG_X, IMG_Y, 3)),\n",
    "    # First Conv Block\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Second Conv Block\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Third Conv Block\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Fourth Conv Block\n",
    "    tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Flatten to convert feature maps to 1D\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),  # Reduces overfitting\n",
    "\n",
    "    # Output Layer - 75 Classes\n",
    "    tf.keras.layers.Dense(number_of_labels, activation='softmax')  # Softmax for multi-class classification\n",
    "])\n",
    "\n",
    "# Defining the loss function\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 561ms/step - accuracy: 0.1094 - loss: 16.7602 - val_accuracy: 0.2417 - val_loss: 2.1005\n",
      "Epoch 2/20\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 563ms/step - accuracy: 0.2715 - loss: 2.0656 - val_accuracy: 0.3892 - val_loss: 1.7979\n",
      "Epoch 3/20\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 734ms/step - accuracy: 0.4356 - loss: 1.6454 - val_accuracy: 0.4438 - val_loss: 1.6147\n",
      "Epoch 4/20\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 556ms/step - accuracy: 0.5597 - loss: 1.2903 - val_accuracy: 0.4795 - val_loss: 1.5567\n",
      "Epoch 5/20\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 556ms/step - accuracy: 0.6763 - loss: 0.9647 - val_accuracy: 0.4771 - val_loss: 1.7268\n",
      "Epoch 6/20\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 557ms/step - accuracy: 0.7647 - loss: 0.7005 - val_accuracy: 0.4990 - val_loss: 1.9750\n",
      "Epoch 7/20\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 558ms/step - accuracy: 0.8361 - loss: 0.5015 - val_accuracy: 0.4971 - val_loss: 2.1906\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20  # Adjust based on performance\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=3, restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 150ms/step - accuracy: 0.4836 - loss: 1.5278\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining the hyperparameters we would tune, and their values to be tested\n",
    "# HP_FILTER_SIZE = hp.HParam('filter_size', hp.Discrete([3,5,7]))\n",
    "# HP_FILTER_NUM = hp.HParam('filters_number', hp.Discrete([32,64,96,128]))\n",
    "\n",
    "# METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "# # Logging setup info\n",
    "# with tf.summary.create_file_writer(r'logs/model-1/hparam_tuning/').as_default():\n",
    "#     hp.hparams_config(\n",
    "#         hparams=[HP_FILTER_SIZE, HP_FILTER_NUM],\n",
    "#         metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_EPOCHS = 20  # Adjust based on performance\n",
    "# BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Wrapping our model and training in a function\n",
    "# def train_test_model(hparams, session_num):\n",
    "    \n",
    "#     # Outlining the model/architecture of our CNN\n",
    "#     model = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Input(shape=(IMG_X, IMG_Y, 3)),\n",
    "#         # First Conv Block\n",
    "#         tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "#         tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "#         # Second Conv Block\n",
    "#         tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "#         tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "#         # Third Conv Block\n",
    "#         tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "#         tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "#         # Fourth Conv Block\n",
    "#         tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), activation='relu'),\n",
    "#         tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "#         # Flatten to convert feature maps to 1D\n",
    "#         tf.keras.layers.Flatten(),\n",
    "\n",
    "#         # Fully Connected Layer\n",
    "#         tf.keras.layers.Dense(512, activation='relu'),\n",
    "#         tf.keras.layers.Dropout(0.3),  # Reduces overfitting\n",
    "\n",
    "#         # Output Layer - 75 Classes\n",
    "#         tf.keras.layers.Dense(75, activation='softmax')  # Softmax for multi-class classification\n",
    "#     ])\n",
    "    \n",
    "#     # Defining the loss function\n",
    "#     loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "#     # Compiling the model\n",
    "#     model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "#     # Defining the logging directory\n",
    "#     log_dir = f\"logs/model-1/fit/run-{session_num}\"\n",
    "    \n",
    "#     # Define the Tensorboard and Confusion Matrix callbacks.\n",
    "#     tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)\n",
    "\n",
    "    \n",
    "#     # Defining early stopping to prevent overfitting\n",
    "#     early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "#         monitor = 'val_loss',\n",
    "#         mode = 'auto',\n",
    "#         min_delta = 0,\n",
    "#         patience = 3,\n",
    "#         verbose = 0, \n",
    "#         restore_best_weights = True\n",
    "#     )\n",
    "    \n",
    "#     # Training the model\n",
    "#     model.fit(\n",
    "#         train_dataset,\n",
    "#         # images_train,\n",
    "#         # labels_train,\n",
    "#         epochs = NUM_EPOCHS,\n",
    "#         batch_size = BATCH_SIZE,\n",
    "#         callbacks = [tensorboard_callback, early_stopping],\n",
    "#         validation_data = (val_dataset),\n",
    "#         # validation_data = (images_val,labels_val),\n",
    "#         verbose = 2\n",
    "#     )\n",
    "    \n",
    "    \n",
    "#     # Evaluating the model's performance on the validation set\n",
    "#     _, accuracy = model.evaluate(val_dataset)\n",
    "    \n",
    "#     # Saving the current model for future reference\n",
    "#     model.save(f\"saved_models/model-1/run-{session_num}.keras\")\n",
    "    \n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating a function to log the resuls\n",
    "# def run(log_dir, hparams, session_num):\n",
    "    \n",
    "#     with tf.summary.create_file_writer(log_dir).as_default():\n",
    "#         hp.hparams(hparams)  # record the values used in this trial\n",
    "#         accuracy = train_test_model(hparams, session_num)\n",
    "#         tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session_num = 1\n",
    "\n",
    "# for filter_size in HP_FILTER_SIZE.domain.values:\n",
    "#     for filter_num in HP_FILTER_NUM.domain.values:\n",
    "\n",
    "#         hparams = {\n",
    "#             HP_FILTER_SIZE: filter_size,\n",
    "#             HP_FILTER_NUM: filter_num\n",
    "#         }\n",
    "\n",
    "#         run_name = f\"run-{session_num}\"\n",
    "#         print(f'--- Starting trial: {run_name}')\n",
    "#         print({h.name: hparams[h] for h in hparams})\n",
    "#         run('logs/model-1/hparam_tuning/' + run_name, hparams, session_num)\n",
    "\n",
    "#         session_num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
