{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import html\n",
    "import string\n",
    "from collections import Counter\n",
    "from itertools import chain, combinations\n",
    "from nltk import word_tokenize, FreqDist, bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from category_encoders import TargetEncoder as ce_TargetEncoder\n",
    "from sklearn.preprocessing import TargetEncoder as skl_TargetEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# from fuzzywuzzy import process\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Disasters:\\t{df_train[df_train.target==1].shape[0]} ({round(df_train[df_train.target==1].shape[0]/df_train.shape[0]*100,1)}%)')\n",
    "print(f'Not disasters:\\t{df_train[df_train.target==0].shape[0]} ({round(df_train[df_train.target==0].shape[0]/df_train.shape[0]*100,1)}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['keyword'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train[df_train['target']==1][['keyword','target']].groupby('keyword').value_counts().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train[df_train['target']==0][['keyword','target']].groupby('keyword').value_counts().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('keyword', as_index=False).agg(target_mean=('target','mean'), keyword_count=('target','size')).query('target_mean > 0.95').sort_values('target_mean', ascending=False).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('keyword', as_index=False).agg(target_mean=('target','mean'), keyword_count=('target','size')).query('target_mean <0.05').sort_values('target_mean', ascending=True).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['location'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train[['location','target']].groupby('location').value_counts())\n",
    "# grouped_counts = df_train[['location','target']].groupby('location').value_counts()\n",
    "# grouped_counts[grouped_counts > 10].index.get_level_values('location').unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train[df_train['target']==1][['location','target']].groupby('location').value_counts().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train[df_train['target']==0][['location','target']].groupby('location').value_counts().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('location', as_index=False).agg(target_mean=('target','mean'), keyword_count=('target','size')).query('target_mean > 0.95').sort_values('target_mean', ascending=False).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('location', as_index=False).agg(target_mean=('target','mean'), keyword_count=('target','size')).query('target_mean < 0.05').sort_values('target_mean', ascending=False).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('location', as_index=False)['target'].mean().query('target > 0.75 & target < 1.0').sort_values('target', ascending=False)\n",
    "# ['location'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('location', as_index=False)['target'].mean().query('target > 0.0 & target < 0.25').sort_values('target', ascending=True)\n",
    "# ['location'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('location', as_index=False).agg(target_mean=('target','mean'), keyword_count=('target','size')).query('keyword_count >= 5').sort_values(['target_mean','keyword_count'], ascending=[False,False]).head(5).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('location', as_index=False).agg(target_mean=('target','mean'), keyword_count=('target','size')).query('keyword_count >= 5').sort_values(['target_mean','keyword_count'], ascending=[True,False]).head(5).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['cleaned_location'] = df_train['location']\n",
    "# df_train['cleaned_location'] = df_train['cleaned_location'].fillna('unknown')\n",
    "\n",
    "# standard_locations = ['Canada','Florida','India','Kenya','London','Los Angeles, CA','Mumbai','New York','Nigeria','UK','USA','United States','Washington, DC','Oklahoma City, OK','Paterson, New Jersey','Lagos, Nigeria','Melbourne, Australia','Puerto Rico','The Netherlands','Nashville, TN','London, England','California, United States','NYC','Brooklyn, NY','Brasil','Boston, MA','San Jose, CA','New York, USA','New Jersey','Vancouver, BC','Manchester']\n",
    "\n",
    "# def generate_location_mapping(train_locations, standard_locations):\n",
    "#     location_mapping = {}\n",
    "#     for loc in train_locations:\n",
    "#         match, score = process.extractOne(loc, standard_locations)\n",
    "#         location_mapping[loc] = match if score > 90 else loc\n",
    "#     return location_mapping\n",
    "\n",
    "# unique_train_locations = df_train['cleaned_location'].unique()\n",
    "# location_mapping = generate_location_mapping(unique_train_locations, standard_locations)\n",
    "\n",
    "# def clean_locations(location, mapping):\n",
    "#     return mapping.get(location, location)\n",
    "\n",
    "# df_train['cleaned_location'] = df_train['cleaned_location'].apply(lambda x: clean_locations(x, location_mapping))\n",
    "\n",
    "# df_train[(df_train['cleaned_location'] != df_train['location']) & (df_train['cleaned_location'] != 'unknown')][['location','cleaned_location']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newlines(text): return re.sub(r'\\n', ' ', text).strip()\n",
    "\n",
    "def fix_html_entities(text): return html.unescape(text)\n",
    "\n",
    "def extract_elements(text, element_type):\n",
    "    patterns = {  'hashtags': r'#[A-Za-z0-9-_]+',\n",
    "                  'handles': r'@[A-Za-z0-9-_]+',\n",
    "                  'urls': r'https?://t.co/[A-Za-z0-9]{10}'  }\n",
    "    pattern = re.compile(patterns[element_type])\n",
    "    elements = pattern.findall(text)\n",
    "    n = len(elements)\n",
    "    elements_str = ' '.join(elements).lower()\n",
    "    new_text = pattern.sub('', text)\n",
    "    return new_text.strip(), elements_str, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text_clean'] = df_train['text'].apply(lambda x: remove_newlines(x))\n",
    "df_train['text_clean'] = df_train['text_clean'].apply(lambda x: fix_html_entities(x))\n",
    "df_train[['text_clean', 'hashtags', 'n_hashtags']] = df_train['text_clean'].apply(lambda x: extract_elements(x,'hashtags')).apply(pd.Series)\n",
    "df_train[['text_clean', 'handles', 'n_handles']] = df_train['text_clean'].apply(lambda x: extract_elements(x,'handles')).apply(pd.Series)\n",
    "df_train[['text_clean', 'urls', 'n_urls']] = df_train['text_clean'].apply(lambda x: extract_elements(x,'urls')).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[(df_train['text_clean'] != df_train['text']) & (df_train['n_hashtags'] >= 2) & (df_train['n_handles'] >= 1) & (df_train['n_urls'] >= 1)][['text','text_clean','hashtags','n_hashtags','handles','n_handles','urls','n_urls']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for element in ['hashtags', 'handles', 'urls']:\n",
    "#     mlb = MultiLabelBinarizer()\n",
    "#     one_hot = pd.DataFrame(mlb.fit_transform(df_train[element]), columns=mlb.classes_, index=df_train.index)\n",
    "#     # df_train = pd.concat([df_train, one_hot], axis=1)\n",
    "#     display(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(' '.join(df_train['text_clean']).lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_clean_text = ' '.join(df_train[df_train['target']==1]['text_clean']).lower()\n",
    "notdisaster_clean_text = ' '.join(df_train[df_train['target']==0]['text_clean']).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltkstopwords = stopwords.words('english')\n",
    "\n",
    "disaster_tokens = [w for w in word_tokenize(disaster_clean_text) if (w not in nltkstopwords) & (w.isalpha())]\n",
    "notdisaster_tokens = [w for w in word_tokenize(notdisaster_clean_text) if (w not in nltkstopwords) & (w.isalpha())]\n",
    "\n",
    "top_disaster_tokens = FreqDist(disaster_tokens).most_common(20)\n",
    "top_notdisaster_tokens = FreqDist(notdisaster_tokens).most_common(20)\n",
    "display(pd.DataFrame(top_disaster_tokens, columns=['Disaster Token', 'Frequency']).head(10))\n",
    "display(pd.DataFrame(top_notdisaster_tokens, columns=['Non-Disaster Token', 'Frequency']).head(10))\n",
    "\n",
    "top_disaster_words = [w for w,f in top_disaster_tokens]\n",
    "top_nondisaster_words = [w for w,f in top_notdisaster_tokens]\n",
    "display(', '.join([w for w in top_disaster_words if w not in top_nondisaster_words]))\n",
    "display(', '.join([w for w in top_nondisaster_words if w not in top_disaster_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_bigrams = [' '.join(b) for b in list(bigrams(disaster_tokens))]\n",
    "nondisaster_bigrams = [' '.join(b) for b in list(bigrams(notdisaster_tokens))]\n",
    "\n",
    "top_disaster_bigrams = FreqDist(disaster_bigrams).most_common(20)\n",
    "top_nondisaster_bigrams = FreqDist(nondisaster_bigrams).most_common(20)\n",
    "display(pd.DataFrame(top_disaster_bigrams, columns=['Disaster Token', 'Frequency']).head(10))\n",
    "display(pd.DataFrame(top_nondisaster_bigrams, columns=['Non-Disaster Token', 'Frequency']).head(10))\n",
    "\n",
    "top_disaster_bigrams = [w for w,f in top_disaster_bigrams]\n",
    "top_nondisaster_bigrams = [w for w,f in top_nondisaster_bigrams]\n",
    "display(' | '.join([w for w in top_disaster_bigrams if w not in top_nondisaster_bigrams]))\n",
    "display(' | '.join([w for w in top_nondisaster_bigrams if w not in top_disaster_bigrams]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(' '.join(df_train['hashtags']).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train[df_train['target']==1][['hashtags','target']].groupby('hashtags').value_counts().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train[df_train['target']==0][['hashtags','target']].groupby('hashtags').value_counts().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(' '.join(df_train['handles']).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train[df_train['target']==1][['handles','target']].groupby('handles').value_counts().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train[df_train['target']==0][['handles','target']].groupby('handles').value_counts().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(' '.join(df_train['urls']).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train[df_train['target']==1][['urls','target']].groupby('urls').value_counts().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train[df_train['target']==0][['urls','target']].groupby('urls').value_counts().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_count(text): return len(text)\n",
    "\n",
    "def word_count(text): return len(text.split())\n",
    "\n",
    "def unique_word_count(text): return len(set(text.split()))\n",
    "\n",
    "def avg_word_length(text): return round(sum(len(word) for word in text.split()) / len(text.split()),3)\n",
    "\n",
    "def punctuation_count(text): return len([n for n in text if n in string.punctuation])\n",
    "\n",
    "def stopwords_count(text): return len([n for n in text.lower() if n in nltkstopwords])\n",
    "\n",
    "def caps_count(text): return sum([1 for n in text if n.isupper()])\n",
    "\n",
    "def repeated_words(text):\n",
    "    word_counts = Counter(text.split())\n",
    "    return ' '.join([word for word, count in word_counts.items() if count > 1 and word.lower() not in nltkstopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['char_count'] = df_train['text_clean'].apply(lambda x: char_count(x))\n",
    "df_train['word_count'] = df_train['text_clean'].apply(lambda x: word_count(x))\n",
    "df_train['unique_word_count'] = df_train['text_clean'].apply(lambda x: unique_word_count(x))\n",
    "df_train['avg_word_length'] = df_train['text_clean'].apply(lambda x: avg_word_length(x))\n",
    "df_train['punctuation_count'] = df_train['text_clean'].apply(lambda x: punctuation_count(x))\n",
    "df_train['stopwords_count'] = df_train['text_clean'].apply(lambda x: stopwords_count(x))\n",
    "df_train['caps_count'] = df_train['text_clean'].apply(lambda x: caps_count(x))\n",
    "# df_train['repeated_words'] = df_train['text_clean'].apply(lambda x: repeated_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['char_count','word_count','unique_word_count','avg_word_length','punctuation_count','stopwords_count','caps_count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_features(df, poly=None):\n",
    "    cols = ['n_handles','n_hashtags','n_urls','char_count','word_count','unique_word_count','avg_word_length','punctuation_count','stopwords_count','caps_count']\n",
    "    numerical_features = df[cols]\n",
    "    if poly is None:\n",
    "        poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "        poly.fit(numerical_features)\n",
    "    poly_features = poly.transform(numerical_features)\n",
    "    poly_feature_names = poly.get_feature_names_out(numerical_features.columns)\n",
    "    df_poly = pd.DataFrame(poly_features, columns=poly_feature_names, index=df.index)\n",
    "    df_poly = df_poly.loc[:, ~df_poly.columns.isin(numerical_features.columns)]\n",
    "    return pd.concat([df, df_poly], axis=1), poly\n",
    "\n",
    "df_train, poly = poly_features(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train.iloc[0][['char_count^2','char_count word_count','char_count unique_word_count','char_count avg_word_length','char_count punctuation_count','char_count stopwords_count','char_count caps_count']]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train.select_dtypes(include=['number']).drop('id', axis=1).corr()['target'].drop('target').sort_values(ascending=False).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['keyword', 'location']\n",
    "# features = ['keyword', 'cleaned_location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_encoder = ce_TargetEncoder(cols=features)\n",
    "ce_transformed_df = ce_encoder.fit_transform(df_train[features], df_train['target']).add_suffix('_target_ce')\n",
    "df_train = df_train.join(ce_transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_encoder = skl_TargetEncoder(categories='auto', target_type='binary', smooth='auto', cv=5, random_state=42)\n",
    "skl_transformed = skl_encoder.fit_transform(df_train[features], df_train['target'])\n",
    "skl_transformed_df = pd.DataFrame(skl_transformed, columns=[f\"{col}_target_skl\" for col in features], index=df_train.index)\n",
    "df_train = df_train.join(skl_transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train[df_train['location']=='London'][['location','location_target_ce','location_target_skl']]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train[df_train['location']=='London'][['target','location_target_skl']].groupby('location_target_skl').value_counts()).sort_values(['location_target_skl','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train[df_train['location']=='London'][['target','location_target_skl']].groupby('target').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction: CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_hashtags = CountVectorizer(min_df=4)\n",
    "df_train_hashtags_vectorised = vec_hashtags.fit_transform(df_train['hashtags'])\n",
    "df_train_hashtags_vectorised_df = pd.DataFrame(df_train_hashtags_vectorised.toarray(), columns=vec_hashtags.get_feature_names_out())\n",
    "\n",
    "vec_handles = CountVectorizer(min_df=2)\n",
    "df_train_handles_vectorised = vec_handles.fit_transform(df_train['handles'])\n",
    "df_train_handles_vectorised_df = pd.DataFrame(df_train_handles_vectorised.toarray(), columns=vec_handles.get_feature_names_out())\n",
    "\n",
    "vec_urls = CountVectorizer(min_df=2, token_pattern=r'https?://t.co/[A-Za-z0-9]{10}')\n",
    "df_train_urls_vectorised = vec_urls.fit_transform(df_train['urls'])\n",
    "df_train_urls_vectorised_df = pd.DataFrame(df_train_urls_vectorised.toarray(), columns=vec_urls.get_feature_names_out())\n",
    "\n",
    "print(f'{df_train_hashtags_vectorised_df.shape[1]} {df_train_handles_vectorised_df.shape[1]} {df_train_urls_vectorised_df.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train_hashtags_vectorised_df.transpose().dot(df_train['target']) / df_train_hashtags_vectorised_df.sum(axis=0)).sort_values(0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train_handles_vectorised_df.transpose().dot(df_train['target']) / df_train_handles_vectorised_df.sum(axis=0)).sort_values(0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train_urls_vectorised_df.transpose().dot(df_train['target']) / df_train_urls_vectorised_df.sum(axis=0)).sort_values(0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.join(df_train_hashtags_vectorised_df, rsuffix='_hashtags')\n",
    "df_train = df_train.join(df_train_handles_vectorised_df, rsuffix='_handles')\n",
    "df_train = df_train.join(df_train_urls_vectorised_df, rsuffix='_urls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction: TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['word_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_text = TfidfVectorizer(min_df=10, ngram_range=(1,5), stop_words='english') \n",
    "df_train_text_clean_vectorised = vec_text.fit_transform(df_train['text_clean'])\n",
    "df_train_text_clean_vectorised_df = pd.DataFrame(df_train_text_clean_vectorised.toarray(), columns=vec_text.get_feature_names_out())\n",
    "\n",
    "print(df_train_text_clean_vectorised_df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_train_text_clean_vectorised_df.transpose().dot(df_train['target']) / df_train_text_clean_vectorised_df.sum(axis=0)).sort_values(0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.join(df_train_text_clean_vectorised_df, rsuffix='_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=42, solver='liblinear')\n",
    "\n",
    "cv_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_to_drop = df_train.select_dtypes(exclude=['number']).columns.to_list()\n",
    "# features_to_drop.extend(['id'])\n",
    "\n",
    "# X_train = df_train.drop(columns=features_to_drop+['target'])\n",
    "# y_train = df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_stats = ['char_count','word_count','unique_word_count','avg_word_length','punctuation_count','stopwords_count','caps_count','n_handles','n_hashtags','n_urls',]\n",
    "features_polys = list(poly.get_feature_names_out())\n",
    "features_te_ce = ['keyword_target_ce','location_target_ce']\n",
    "features_te_skl = ['keyword_target_skl','location_target_skl']\n",
    "features_cv_hashtags = list(vec_hashtags.get_feature_names_out())\n",
    "features_cv_handles = list(vec_handles.get_feature_names_out())\n",
    "features_cv_urls = list(vec_urls.get_feature_names_out())\n",
    "features_cv = features_cv_hashtags + features_cv_handles + features_cv_urls\n",
    "features_tv = list(vec_text.get_feature_names_out())\n",
    "\n",
    "features_to_keep = features_stats + features_te_ce + features_te_skl + features_cv + features_tv\n",
    "features_to_keep.remove('text')\n",
    "\n",
    "X_train = df_train[features_to_keep]\n",
    "y_train = df_train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_f1 = cross_val_score(lr, X_train, y_train, cv=5, scoring='f1').mean()\n",
    "print(f'Cross-validated F1 score:\\t{round(cross_val_f1,4)}')\n",
    "cv_scores.append(('Initial',cross_val_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE (pre-scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Positives: {df_train[df_train.target==1].shape[0]} ({round(df_train[df_train.target==1].shape[0]/df_train.shape[0]*100,1)}%)')\n",
    "print(f'Negatives: {df_train[df_train.target==0].shape[0]} ({round(df_train[df_train.target==0].shape[0]/df_train.shape[0]*100,1)}%)')\n",
    "print(f'X number of rows: {X_train.shape[0]}')\n",
    "print()\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f'Positives: {int(y_train[y_train==1].count())} ({round(int(y_train[y_train==1].count())/int(y_train.count())*100,1)}%)')\n",
    "print(f'Negatives: {int(y_train[y_train==0].count())} ({round(int(y_train[y_train==0].count())/int(y_train.count())*100,1)}%)')\n",
    "print(f'X number of rows: {y_train.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_f1 = cross_val_score(lr, X_train, y_train, cv=5, scoring='f1').mean()\n",
    "print(f'Cross-validated F1 score:\\t{round(cross_val_f1,4)}')\n",
    "cv_scores.append(('SMOTE',cross_val_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_f1 = cross_val_score(lr, X_train_scaled, y_train, cv=5, scoring='f1').mean()\n",
    "print(f'Cross-validated F1 score:\\t{round(cross_val_f1,4)}')\n",
    "cv_scores.append(('Scale',cross_val_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_spaces = [{'solver':['liblinear'], 'penalty':['l1','l2'], 'C':(1e-4, 1e4, 'log-uniform')}]\n",
    "bayessearch_lr = BayesSearchCV(LogisticRegression(random_state=42), search_spaces=search_spaces, n_iter=100, scoring='f1', cv=5, n_jobs=-1)\n",
    "bayessearch_lr.fit(X_train_scaled, y_train)\n",
    "print(\"Best score:\", bayessearch_lr.best_score_)\n",
    "print(\"Best parameters:\", bayessearch_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = [{'solver':['liblinear'], 'penalty':['l1','l2'], 'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000]}]\n",
    "# gridsearch_lr = GridSearchCV(LogisticRegression(random_state=42, max_iter=100), param_grid=param_grid, scoring='f1', cv=5, n_jobs=-1)\n",
    "# gridsearch_lr.fit(X_train_scaled, y_train)\n",
    "# print(\"Best score:\", gridsearch_lr.best_score_)\n",
    "# print(\"Best parameters:\", gridsearch_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = bayessearch_lr.best_estimator_\n",
    "# lr = LogisticRegression(random_state=42, C=0.14421478790765738, penalty='l1', solver='liblinear')\n",
    "# lr = gridsearch_lr.best_estimator_\n",
    "# lr = LogisticRegression(random_state=42, C=0.1, penalty='l1', solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_f1 = cross_val_score(lr, X_train_scaled, y_train, cv=5, scoring='f1').mean()\n",
    "print(f'Cross-validated F1 score:\\t{round(cross_val_f1,4)}')\n",
    "cv_scores.append(('LR',cross_val_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection: Select K Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_pipeline = Pipeline([('select',SelectKBest(score_func=chi2)), ('clf',lr)])\n",
    "bayes_search_selector = BayesSearchCV(estimator=selector_pipeline, search_spaces={'select__k':(1,X_train_scaled.shape[1])}, n_iter=50, scoring='f1', cv=5, verbose=0, n_jobs=-1)\n",
    "bayes_search_selector.fit(X_train_scaled, y_train)\n",
    "print(\"Best k:\", bayes_search_selector.best_params_['select__k'])\n",
    "print(\"Best F1 score:\", bayes_search_selector.best_score_)\n",
    "selector_kb = bayes_search_selector.best_estimator_[0]\n",
    "# selector_kb = SelectKBest(score_func=chi2, k=500)\n",
    "X_train_scaled = selector_kb.fit_transform(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_f1 = cross_val_score(lr, X_train_scaled, y_train, cv=5, scoring='f1').mean()\n",
    "print(f'Cross-validated F1 score:\\t{round(cross_val_f1,4)}')\n",
    "cv_scores.append(('Select K Best',cross_val_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection: Variance Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "# selector_vt = VarianceThreshold(threshold=0.01)\n",
    "# X_train_scaled = selector_vt.fit_transform(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection: RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv = RFECV(estimator=lr, step=10, cv=5, scoring='f1')\n",
    "rfecv.fit(X_train_scaled, y_train)\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Number of correct classifications)\")\n",
    "plt.plot(rfecv.cv_results_['n_features'], rfecv.cv_results_['mean_test_score'])\n",
    "plt.show()\n",
    "\n",
    "print(\"Optimal number of features:\", rfecv.n_features_)\n",
    "# rfecv_features = rfecv.support_\n",
    "# print(\"Selected features:\", rfecv_features)\n",
    "# print(\"Selected features:\", X_train.columns[rfecv_features])\n",
    "# # print(\"Feature rankings:\", rfecv.ranking_)\n",
    "\n",
    "X_train_scaled = rfecv.transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_f1 = cross_val_score(lr, X_train_scaled, y_train, cv=5, scoring='f1').mean()\n",
    "print(f'Cross-validated F1 score:\\t{round(cross_val_f1,4)}')\n",
    "cv_scores.append(('RFECV',cross_val_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: SMOTE (post-scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Positives: {df_train[df_train.target==1].shape[0]} ({round(df_train[df_train.target==1].shape[0]/df_train.shape[0]*100,1)}%)')\n",
    "# print(f'Negatives: {df_train[df_train.target==0].shape[0]} ({round(df_train[df_train.target==0].shape[0]/df_train.shape[0]*100,1)}%)')\n",
    "# print(f'X number of rows: {X_train.shape[0]}')\n",
    "# print()\n",
    "\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_train_scaled, y_train = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# print(f'Positives: {int(y_train[y_train==1].count())} ({round(int(y_train[y_train==1].count())/int(y_train.count())*100,1)}%)')\n",
    "# print(f'Negatives: {int(y_train[y_train==0].count())} ({round(int(y_train[y_train==0].count())/int(y_train.count())*100,1)}%)')\n",
    "# print(f'X number of rows: {y_train.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_val_f1 = cross_val_score(lr, X_train_scaled, y_train, cv=5, scoring='f1').mean()\n",
    "# print(f'Cross-validated F1 score:\\t{round(cross_val_f1,4)}')\n",
    "# cv_scores.append(('smote',cross_val_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validated F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cv_scores, columns=['stage', 'f1']).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     ('smote', SMOTE(random_state=42)),\n",
    "#     ('scaler', MinMaxScaler()),\n",
    "#     ('feature_selection', SelectKBest(score_func=chi2)),\n",
    "#     ('clf', LogisticRegression(random_state=42))\n",
    "# ])\n",
    "\n",
    "# param_grid = {\n",
    "#     'feature_selection__k': (1, X_train.shape[1]),\n",
    "#     'clf__solver': ['liblinear'],\n",
    "#     'clf__penalty': ['l1', 'l2'],\n",
    "#     'clf__C': (1e-4, 1e4, 'log-uniform')\n",
    "# }\n",
    "\n",
    "# bayes_search = BayesSearchCV(\n",
    "#     estimator=pipeline,\n",
    "#     search_spaces=param_grid,\n",
    "#     n_iter=100,\n",
    "#     scoring='f1',\n",
    "#     cv=10,\n",
    "#     n_jobs=-1,\n",
    "#     verbose=0\n",
    "# )\n",
    "\n",
    "# bayes_search.fit(X_train, y_train)\n",
    "# best_pipeline = bayes_search.best_estimator_\n",
    "\n",
    "# rfecv = RFECV(estimator=best_pipeline.named_steps['clf'], step=5, cv=10, scoring='f1')\n",
    "# X_train_rfecv = rfecv.fit_transform(best_pipeline[:-1].fit_transform(X_train, y_train), y_train)\n",
    "\n",
    "# cross_val_f1 = cross_val_score(best_pipeline.named_steps['clf'], X_train_scaled, y_train, cv=10, scoring='f1').mean()\n",
    "# print(f'Cross-validated F1 score:\\t{round(cross_val_f1,4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr.fit(X_train_scaled, y_train)\n",
    "# y_test = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Training F1 score:\\t{round(f1_score(y_train, lr.predict(X_train_scaled)),4)}')\n",
    "# print(f'Training accuracy:\\t{round(lr.score(X_train_scaled, y_train),4)}')\n",
    "# cm = confusion_matrix(y_train, lr.predict(X_train_scaled))\n",
    "# display(pd.DataFrame(cm,index=['Actual Negative', 'Actual Positive'],columns=['Predicted Negative', 'Predicted Positive']))\n",
    "# display(pd.DataFrame((cm/cm.sum()*100).round(1),index=['Actual Negative (%)', 'Actual Positive (%)'],columns=['Predicted Negative (%)', 'Predicted Positive (%)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# print(f'Cross-validated F1 score:\\t{round(cross_val_score(lr, X_train_scaled, y_train, cv=5, scoring='f1').mean(),4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_test['cleaned_location'] = df_test['location']\n",
    "# # df_test['cleaned_location'] = df_test['cleaned_location'].fillna('unknown')\n",
    "# # df_test['cleaned_location'] = df_test['cleaned_location'].apply(lambda x: clean_locations(x, location_mapping))\n",
    "\n",
    "# df_test['text_clean'] = df_test['text'].apply(lambda x: remove_newlines(x))\n",
    "# df_test['text_clean'] = df_test['text_clean'].apply(lambda x: fix_html_entities(x))\n",
    "# df_test[['text_clean', 'hashtags', 'n_hashtags']] = df_test['text_clean'].apply(lambda x: extract_elements(x,'hashtags')).apply(pd.Series)\n",
    "# df_test[['text_clean', 'handles', 'n_handles']] = df_test['text_clean'].apply(lambda x: extract_elements(x,'handles')).apply(pd.Series)\n",
    "# df_test[['text_clean', 'urls', 'n_urls']] = df_test['text_clean'].apply(lambda x: extract_elements(x,'urls')).apply(pd.Series)\n",
    "# df_test['char_count'] = df_test['text_clean'].apply(lambda x: char_count(x))\n",
    "# df_test['word_count'] = df_test['text_clean'].apply(lambda x: word_count(x))\n",
    "# df_test['unique_word_count'] = df_test['text_clean'].apply(lambda x: unique_word_count(x))\n",
    "# df_test['avg_word_length'] = df_test['text_clean'].apply(lambda x: avg_word_length(x))\n",
    "# df_test['punctuation_count'] = df_test['text_clean'].apply(lambda x: punctuation_count(x))\n",
    "# df_test['stopwords_count'] = df_test['text_clean'].apply(lambda x: stopwords_count(x))\n",
    "# df_test['caps_count'] = df_test['text_clean'].apply(lambda x: caps_count(x))\n",
    "\n",
    "# df_test, _ = poly_features(df_test, poly=poly)\n",
    "\n",
    "# df_test = df_test.join(ce_encoder.transform(df_test[features]).add_suffix('_target_ce'))\n",
    "# df_test = df_test.join(pd.DataFrame(skl_encoder.transform(df_test[features]), columns=[f\"{col}_target_skl\" for col in features], index=df_test.index))\n",
    "\n",
    "# df_test_hashtags_vectorised = vec_hashtags.transform(df_test['hashtags'])\n",
    "# df_test_hashtags_vectorised_df = pd.DataFrame(df_test_hashtags_vectorised.toarray(), columns=vec_hashtags.get_feature_names_out())\n",
    "# df_test_handles_vectorised = vec_handles.transform(df_test['handles'])\n",
    "# df_test_handles_vectorised_df = pd.DataFrame(df_test_handles_vectorised.toarray(), columns=vec_handles.get_feature_names_out())\n",
    "# df_test_urls_vectorised = vec_urls.transform(df_test['urls'])\n",
    "# df_test_urls_vectorised_df = pd.DataFrame(df_test_urls_vectorised.toarray(), columns=vec_urls.get_feature_names_out())\n",
    "# df_test_text_clean_vectorised = vec_text.transform(df_test['text_clean'])\n",
    "# df_test_text_clean_vectorised_df = pd.DataFrame(df_test_text_clean_vectorised.toarray(), columns=vec_text.get_feature_names_out())\n",
    "\n",
    "# df_test = df_test.join(df_test_hashtags_vectorised_df, rsuffix='_urls')\n",
    "# df_test = df_test.join(df_test_handles_vectorised_df, rsuffix='_handles')\n",
    "# df_test = df_test.join(df_test_urls_vectorised_df, rsuffix='_hashtags')\n",
    "# df_test = df_test.join(df_test_text_clean_vectorised_df, rsuffix='_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = df_test.drop(columns=features_to_drop)\n",
    "# X_test = df_test[features_to_keep]\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "# X_test_scaled = selector_kb.transform(X_test_scaled)\n",
    "# X_test_scaled = selector_vt.transform(X_test_scaled)\n",
    "# X_test_scaled = rfecv.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr.fit(X_train, y_train)\n",
    "# y_pred = lr.predict(X_test)\n",
    "# submission['target'] = y_pred\n",
    "# print(submission.shape)\n",
    "# submission.to_csv('submission_jg_XXX.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substring = 'Deeds'\n",
    "# matches = X_train.astype(str).apply(lambda col: col.str.contains(substring, na=False))\n",
    "# filtered_rows = X_train[matches.any(axis=1)]\n",
    "# print(\"Rows with matches:\\n\", filtered_rows)\n",
    "# locations = matches.stack()[matches.stack()]\n",
    "# print(\"Locations of matches:\\n\", locations.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = ['features_stats', 'features_polys', 'features_te_ce', 'features_te_skl', 'features_cv_hashtags', 'features_cv_handles', 'features_cv_urls', 'features_tv']\n",
    "\n",
    "# def all_combinations(iterable):\n",
    "#     return chain.from_iterable(combinations(iterable, r) for r in range(1, len(iterable) + 1))\n",
    "\n",
    "# feature_combinations = list(all_combinations(features))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
